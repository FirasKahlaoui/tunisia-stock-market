{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout # type: ignore\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/firas/.local/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">80,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">80,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m40,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m80,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m80,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m101\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">201,701</span> (787.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m201,701\u001b[0m (787.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">201,701</span> (787.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m201,701\u001b[0m (787.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load the CSV data\n",
    "df = pd.read_csv('./data/cleaned_weekly_stock_market.csv')\n",
    "\n",
    "# Filter the data for \"AMEN BANK\"\n",
    "amen_bank = df[df['companyName'] == 'AMEN BANK']\n",
    "\n",
    "# Extract closing prices\n",
    "amen_bank = amen_bank.reset_index()['closingPrice']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(np.array(amen_bank).reshape(-1,1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "training_size = int(len(scaled_data) * 0.70)  # Increase training size\n",
    "test_size = len(scaled_data) - training_size\n",
    "train_data, test_data = scaled_data[0:training_size,:], scaled_data[training_size:len(scaled_data),:1]\n",
    "\n",
    "# Function to create a dataset matrix\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        a = dataset[i:(i + time_step), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# Create datasets\n",
    "time_step = 15  # Increase time step\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build the LSTM model with dropout for regularization\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(time_step, 1)))\n",
    "model.add(Dropout(0.2))  # Add dropout layer\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(Dropout(0.2))  # Add dropout layer\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 111ms/step - loss: 0.0117 - val_loss: 0.0487\n",
      "Epoch 2/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0031 - val_loss: 0.0240\n",
      "Epoch 3/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.0026 - val_loss: 0.0134\n",
      "Epoch 4/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0026 - val_loss: 0.0217\n",
      "Epoch 5/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0023 - val_loss: 0.0172\n",
      "Epoch 6/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0019 - val_loss: 0.0099\n",
      "Epoch 7/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0018 - val_loss: 0.0076\n",
      "Epoch 8/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.0017 - val_loss: 0.0089\n",
      "Epoch 9/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0015 - val_loss: 0.0140\n",
      "Epoch 10/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0018 - val_loss: 0.0038\n",
      "Epoch 11/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0016 - val_loss: 0.0088\n",
      "Epoch 12/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0014 - val_loss: 0.0064\n",
      "Epoch 13/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.0014 - val_loss: 0.0112\n",
      "Epoch 14/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0012 - val_loss: 0.0128\n",
      "Epoch 15/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.0013 - val_loss: 0.0109\n",
      "Epoch 16/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0012 - val_loss: 0.0098\n",
      "Epoch 17/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0012 - val_loss: 0.0052\n",
      "Epoch 18/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0012 - val_loss: 0.0079\n",
      "Epoch 19/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0012 - val_loss: 0.0136\n",
      "Epoch 20/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0014 - val_loss: 0.0085\n",
      "Epoch 21/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0011 - val_loss: 0.0084\n",
      "Epoch 22/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0011 - val_loss: 0.0081\n",
      "Epoch 23/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 8.7986e-04 - val_loss: 0.0032\n",
      "Epoch 24/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0010 - val_loss: 0.0049\n",
      "Epoch 25/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 9.0781e-04 - val_loss: 0.0041\n",
      "Epoch 26/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 9.4903e-04 - val_loss: 0.0045\n",
      "Epoch 27/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 8.7173e-04 - val_loss: 0.0060\n",
      "Epoch 28/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 8.9559e-04 - val_loss: 0.0061\n",
      "Epoch 29/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 7.6340e-04 - val_loss: 0.0046\n",
      "Epoch 30/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 9.9725e-04 - val_loss: 0.0039\n",
      "Epoch 31/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 8.5110e-04 - val_loss: 0.0035\n",
      "Epoch 32/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 8.6545e-04 - val_loss: 0.0031\n",
      "Epoch 33/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 8.3897e-04 - val_loss: 0.0053\n",
      "Epoch 34/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 8.5229e-04 - val_loss: 0.0050\n",
      "Epoch 35/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 8.2124e-04 - val_loss: 0.0076\n",
      "Epoch 36/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 8.0745e-04 - val_loss: 0.0067\n",
      "Epoch 37/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 8.6199e-04 - val_loss: 0.0066\n",
      "Epoch 38/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 7.2949e-04 - val_loss: 0.0072\n",
      "Epoch 39/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 7.6958e-04 - val_loss: 0.0026\n",
      "Epoch 40/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 7.6642e-04 - val_loss: 0.0022\n",
      "Epoch 41/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 8.5307e-04 - val_loss: 0.0024\n",
      "Epoch 42/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 6.9217e-04 - val_loss: 0.0024\n",
      "Epoch 43/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 6.8636e-04 - val_loss: 0.0030\n",
      "Epoch 44/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 6.9846e-04 - val_loss: 0.0029\n",
      "Epoch 45/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 6.1433e-04 - val_loss: 0.0037\n",
      "Epoch 46/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 6.2124e-04 - val_loss: 0.0040\n",
      "Epoch 47/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - loss: 6.2470e-04 - val_loss: 0.0038\n",
      "Epoch 48/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 6.4008e-04 - val_loss: 0.0024\n",
      "Epoch 49/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 6.9291e-04 - val_loss: 0.0017\n",
      "Epoch 50/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 6.1306e-04 - val_loss: 0.0020\n",
      "Epoch 51/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 6.1486e-04 - val_loss: 0.0019\n",
      "Epoch 52/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 6.1212e-04 - val_loss: 0.0040\n",
      "Epoch 53/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 5.5369e-04 - val_loss: 0.0013\n",
      "Epoch 54/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 5.6934e-04 - val_loss: 0.0041\n",
      "Epoch 55/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 6.1513e-04 - val_loss: 0.0038\n",
      "Epoch 56/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 6.9290e-04 - val_loss: 0.0029\n",
      "Epoch 57/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 6.2013e-04 - val_loss: 0.0026\n",
      "Epoch 58/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 5.8614e-04 - val_loss: 0.0037\n",
      "Epoch 59/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 5.0338e-04 - val_loss: 0.0028\n",
      "Epoch 60/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - loss: 5.2574e-04 - val_loss: 0.0032\n",
      "Epoch 61/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 4.9990e-04 - val_loss: 0.0036\n",
      "Epoch 62/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 6.7570e-04 - val_loss: 0.0018\n",
      "Epoch 63/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 5.4332e-04 - val_loss: 0.0025\n",
      "Epoch 64/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 5.9790e-04 - val_loss: 0.0018\n",
      "Epoch 65/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 5.0979e-04 - val_loss: 0.0013\n",
      "Epoch 66/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 4.8088e-04 - val_loss: 0.0012\n",
      "Epoch 67/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 5.5247e-04 - val_loss: 0.0015\n",
      "Epoch 68/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 5.2468e-04 - val_loss: 0.0019\n",
      "Epoch 69/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 4.6756e-04 - val_loss: 0.0024\n",
      "Epoch 70/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 4.5501e-04 - val_loss: 0.0018\n",
      "Epoch 71/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 5.8363e-04 - val_loss: 0.0021\n",
      "Epoch 72/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 5.3599e-04 - val_loss: 0.0012\n",
      "Epoch 73/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 4.6867e-04 - val_loss: 0.0023\n",
      "Epoch 74/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 4.2503e-04 - val_loss: 0.0016\n",
      "Epoch 75/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 5.2362e-04 - val_loss: 0.0019\n",
      "Epoch 76/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 5.1148e-04 - val_loss: 0.0017\n",
      "Epoch 77/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - loss: 5.0134e-04 - val_loss: 0.0015\n",
      "Epoch 78/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 5.2498e-04 - val_loss: 9.1502e-04\n",
      "Epoch 79/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 4.2916e-04 - val_loss: 0.0014\n",
      "Epoch 80/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 5.0295e-04 - val_loss: 9.1589e-04\n",
      "Epoch 81/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.4171e-04 - val_loss: 9.4656e-04\n",
      "Epoch 82/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 4.1838e-04 - val_loss: 0.0013\n",
      "Epoch 83/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 4.9117e-04 - val_loss: 0.0020\n",
      "Epoch 84/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - loss: 4.6885e-04 - val_loss: 0.0013\n",
      "Epoch 85/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.5632e-04 - val_loss: 0.0033\n",
      "Epoch 86/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - loss: 5.9602e-04 - val_loss: 9.7224e-04\n",
      "Epoch 87/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - loss: 4.3013e-04 - val_loss: 0.0019\n",
      "Epoch 88/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - loss: 4.9478e-04 - val_loss: 0.0018\n",
      "Epoch 89/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 4.6232e-04 - val_loss: 0.0022\n",
      "Epoch 90/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - loss: 3.9452e-04 - val_loss: 0.0028\n",
      "Epoch 91/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - loss: 4.5472e-04 - val_loss: 0.0020\n",
      "Epoch 92/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 4.3380e-04 - val_loss: 0.0017\n",
      "Epoch 93/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 4.5658e-04 - val_loss: 0.0014\n",
      "Epoch 94/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - loss: 3.6898e-04 - val_loss: 0.0012\n",
      "Epoch 95/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 4.2530e-04 - val_loss: 0.0014\n",
      "Epoch 96/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 4.0358e-04 - val_loss: 0.0027\n",
      "Epoch 97/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 4.6291e-04 - val_loss: 0.0032\n",
      "Epoch 98/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 4.6111e-04 - val_loss: 0.0020\n",
      "Epoch 99/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 4.3076e-04 - val_loss: 0.0020\n",
      "Epoch 100/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - loss: 3.7879e-04 - val_loss: 0.0019\n",
      "Epoch 101/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 4.6004e-04 - val_loss: 0.0025\n",
      "Epoch 102/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - loss: 4.7888e-04 - val_loss: 0.0012\n",
      "Epoch 103/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 3.9252e-04 - val_loss: 0.0022\n",
      "Epoch 104/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 3.4014e-04 - val_loss: 0.0017\n",
      "Epoch 105/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 4.4591e-04 - val_loss: 0.0013\n",
      "Epoch 106/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 4.6863e-04 - val_loss: 0.0010\n",
      "Epoch 107/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 4.4298e-04 - val_loss: 0.0019\n",
      "Epoch 108/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 4.3262e-04 - val_loss: 0.0015\n",
      "Epoch 109/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 4.5501e-04 - val_loss: 0.0016\n",
      "Epoch 110/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 3.9307e-04 - val_loss: 0.0013\n",
      "Epoch 111/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 4.4843e-04 - val_loss: 0.0024\n",
      "Epoch 112/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 4.0248e-04 - val_loss: 0.0021\n",
      "Epoch 113/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - loss: 4.6725e-04 - val_loss: 0.0015\n",
      "Epoch 114/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - loss: 5.1544e-04 - val_loss: 0.0015\n",
      "Epoch 115/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - loss: 3.6998e-04 - val_loss: 0.0020\n",
      "Epoch 116/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 3.3068e-04 - val_loss: 0.0016\n",
      "Epoch 117/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - loss: 4.5721e-04 - val_loss: 0.0018\n",
      "Epoch 118/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 4.0034e-04 - val_loss: 0.0015\n",
      "Epoch 119/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 4.0363e-04 - val_loss: 0.0016\n",
      "Epoch 120/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 4.5376e-04 - val_loss: 0.0014\n",
      "Epoch 121/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 4.1749e-04 - val_loss: 0.0011\n",
      "Epoch 122/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 4.3922e-04 - val_loss: 9.6317e-04\n",
      "Epoch 123/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 5.7432e-04 - val_loss: 0.0012\n",
      "Epoch 124/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - loss: 4.8440e-04 - val_loss: 0.0013\n",
      "Epoch 125/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 4.8890e-04 - val_loss: 0.0019\n",
      "Epoch 126/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 4.1012e-04 - val_loss: 0.0020\n",
      "Epoch 127/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - loss: 4.8832e-04 - val_loss: 0.0019\n",
      "Epoch 128/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 4.5339e-04 - val_loss: 0.0024\n",
      "Epoch 129/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 4.2605e-04 - val_loss: 0.0022\n",
      "Epoch 130/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 4.8678e-04 - val_loss: 0.0026\n",
      "Epoch 131/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - loss: 3.9576e-04 - val_loss: 0.0021\n",
      "Epoch 132/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 4.1511e-04 - val_loss: 0.0018\n",
      "Epoch 133/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 4.3020e-04 - val_loss: 0.0021\n",
      "Epoch 134/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - loss: 5.0346e-04 - val_loss: 0.0019\n",
      "Epoch 135/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 4.5388e-04 - val_loss: 0.0018\n",
      "Epoch 136/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 4.3559e-04 - val_loss: 9.3961e-04\n",
      "Epoch 137/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 4.4642e-04 - val_loss: 0.0013\n",
      "Epoch 138/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 4.1264e-04 - val_loss: 0.0010\n",
      "Epoch 139/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 4.2356e-04 - val_loss: 0.0021\n",
      "Epoch 140/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 4.1031e-04 - val_loss: 9.3438e-04\n",
      "Epoch 141/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 3.9197e-04 - val_loss: 0.0016\n",
      "Epoch 142/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 4.6516e-04 - val_loss: 0.0016\n",
      "Epoch 143/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - loss: 4.3855e-04 - val_loss: 0.0026\n",
      "Epoch 144/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 4.5205e-04 - val_loss: 0.0013\n",
      "Epoch 145/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 4.5687e-04 - val_loss: 0.0016\n",
      "Epoch 146/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 4.2982e-04 - val_loss: 0.0014\n",
      "Epoch 147/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 4.3942e-04 - val_loss: 0.0013\n",
      "Epoch 148/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - loss: 3.1865e-04 - val_loss: 0.0017\n",
      "Epoch 149/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 4.0236e-04 - val_loss: 0.0018\n",
      "Epoch 150/150\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 3.8189e-04 - val_loss: 0.0019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f2f541ced40>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=32, verbose=1)  # Adjust epochs and batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Train RMSE: 21.85214007203231\n",
      "Test RMSE: 27.644141214840634\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make predictions\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "# Inverse transform to get actual values\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "\n",
    "# Calculate RMSE\n",
    "train_rmse = math.sqrt(mean_squared_error(y_train, train_predict))\n",
    "test_rmse = math.sqrt(mean_squared_error(y_test, test_predict))\n",
    "print(f\"Train RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (99,) and (100, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Plot the last 100 actual values and the predicted values for the next 10 days\u001b[39;00m\n\u001b[1;32m     24\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[0;32m---> 25\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mday_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscaled_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mActual Values\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(day_pred, scaler\u001b[38;5;241m.\u001b[39minverse_transform(lst_output), label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Values for Next 10 Days\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStock Price Prediction\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (99,) and (100, 1)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Predict the next 10 days\n",
    "n_steps = time_step\n",
    "x_input = test_data[len(test_data)-n_steps:].reshape(1,-1)\n",
    "temp_input = list(x_input)\n",
    "temp_input = temp_input[0].tolist()\n",
    "\n",
    "lst_output = []\n",
    "for i in range(10):\n",
    "    if len(temp_input) > n_steps:\n",
    "        x_input = np.array(temp_input[1:])\n",
    "        x_input = x_input.reshape(1, -1)\n",
    "        x_input = x_input.reshape((1, n_steps, 1))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        temp_input = temp_input[1:]\n",
    "        lst_output.extend(yhat.tolist())\n",
    "    else:\n",
    "        x_input = x_input.reshape((1, n_steps,1))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        lst_output.extend(yhat.tolist())\n",
    "\n",
    "# Plot the last 100 actual values and the predicted values for the next 10 days\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(day_new, scaler.inverse_transform(scaled_data[len(scaled_data)-100:]), label='Actual Values') #type: ignore\n",
    "plt.plot(day_pred, scaler.inverse_transform(lst_output), label='Predicted Values for Next 10 Days') #type: ignore\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Full visualization including all data points and predictions\n",
    "stock_pred = scaled_data.tolist()\n",
    "stock_pred.extend(lst_output)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(scaler.inverse_transform(stock_pred), label='Full Series with Predictions')\n",
    "plt.title('Full Stock Price Series with Predictions')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
